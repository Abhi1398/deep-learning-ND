# deep-learning-ND
This repository was created for the [Deep Learning Nanodegree](https://www.udacity.com/course/deep-learning-nanodegree--nd101) at [Udacity](https://www.udacity.com/). The original tutorial and project notebooks for this course can be found at the [deep-learning-v2-pytorch](https://github.com/udacity/deep-learning-v2-pytorch) repository.

## Project 1: Predicting Bike-Sharing Patterns

[Predicting Bike-Sharing Patterns Project](project-bikesharing)<br/>

In this project, I build a neural network from scratch to carry out a prediction problem on a real dataset! By building a neural network from the ground up, I have a much better understanding of gradient descent, backpropagation, and other concepts that are important to know before moving on to higher level tools such as PyTorch. The data comes from the [UCI Machine Learning Database](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).


## Project 2: Dog-Breed Classifier

[Dog-Breed Classifier Project](project-dog-classification)<br/>

In this project, I build a pipeline to process real-world, user-supplied images. Given an image of a dog, my algorithm will identify an estimate of the canineâ€™s breed by using state-of-the-art CNN models for classification. If supplied an image of a human face, the code will identify the resembling dog breed.

## Project 3: TV Script Generation
[TV Script Generation Project](project-tv-script-generation)<br/>

In this project, I generate my own Seinfeld TV scripts using a recurrent neural network (RNN). The dataset used to train the network consists of scripts from 9 Seinfeld seasons. After training on the dataset, the RNN can create a new, "fake" TV script.

### Generated script

A script generated by my RNN

>kramer: listen jerry, i'm sorry. i just don't know what it is.
>
>jerry: what?
>
>george: you don't know? how could i go out with her?
>
>jerry: no.
>
>helen: what are you doing?
>
>jerry: what about that?
>
>george:(to the phone) hey. i don't know what you're saying.
>
>jerry: i don't think so.
>
>george: yeah...
>
>jerry: oh, that's right.
>
>george: yeah, right. i was wondering if you could do your own business. it's not that.
>
>jerry: i don't have you.
>
>kramer: no, no, don't you?... no, i want you to be there and i can have the car.
>
>george: oh, oh, i can't believe this isn't a good time.
>
>jerry: what is this?
>
>george:(to the man) oh, i gotta go!
>
>jerry: i got a lot of people in the car.
>
>kramer: yeah.
>
>jerry: hey.
>
>kramer: yeah, you got it, i just want to be a good one.
>
>kramer: yeah, yeah.(she looks at her watch) oh, you know, you got my keys, you were right on the top.
>
>kramer:(to kramer) hey!
>
>kramer: hey, hey jerry.
>
>elaine:(to kramer) hey you guys, what are you doing here?
>
>kramer: i didn't know i said" hello"
>
>jerry:"(to kramer) so, what do you have to say?
>
>jerry: i can't.
>
>george: i just think i'm gonna go to the bathroom and get the rest of my life. i have to do this.
>
>jerry: what are you talking about?
>
>george: why did you get that?
>
>jerry: well, it's like an accident.
>
>jerry: what did you say?
>
>george:(laughing) you got that

## Project 4: Face Generation
[Face Generation Project](project-face-generation)<br/>

In this project, I define and train deep convolutional generative adversarial networks (DCGAN) on a dataset of faces. The goal is to get a generator network to generate new images of faces that look as realistic as possible. The [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) was used to train the adversarial networks.

## Exercises
* [Introduction to Neural Networks](intro-neural-networks) - Implemented gradient descent and backpropagation in python and built a 2-layer Neural Network.
* [Sentiment Analysis Network](sentiment-analysis-network) - In this lesson, Andrew Trask, the author of Grokking Deep Learning, walked me through using neural networks for sentiment analysis.
* [Deep Learning with PyTorch](intro-to-pytorch) - Learned how to use PyTorch for building deep learning models.
* [Convolutional Neural Networks](convolutional-neural-networks) - Implemented CNNs for classification tasks and learned how to visualize the inside of networks.
* [Transfer Learning](transfer-learning) - Learned how to apply a pre-trained network to a new problem with transfer learning.
* [Weight Initialization](weight-initialization) - Learned how to find good initial weights for a neural network.
* [Autoencoder](autoencoder) - Built autoencoders using PyTorch.
* [Style Transfer](style-transfer) - Learned how to use a pre-trained network to extract content and style features from an image. 
* [Implementation of RNN & LSTM](recurrent-neural-networks) - Define and train RNNs in PyTorch and apply them to tasks that involve sequential data. Implemented Character-Level LSTM in PyTorch. 
* [Embeddings & Word2Vec](word2vec-embeddings) - Learned about embeddings in neural networks by implementing the Word2Vec model.
* [Sentiment Prediction RNN](sentiment-rnn) - Implemented a sentiment prediction RNN for predicting whether a movie review is positive or negative. 
* [MNIST GAN](gan-mnist) - Learned to build a generative adversarial network (GAN) trained on the MNIST dataset. From this, we are be able to generate new handwritten digits.
* [Batch Normalization](batch-norm) - This lesson showed how a simple MNIST classification model improves with the addition of batch normalization layers.
* [DCGAN, SVHN](dcgan-svhn) - Learned to build a  Deep Convolutional GAN (DCGAN) that can generate new images that look like house addresses from the Google Streetview dataset, SVHN.
* [Implementing a CycleGAN](cycle-gan) - Implemented a CycleGAN in PyTorch that can translate images from the summer to winter domains.
