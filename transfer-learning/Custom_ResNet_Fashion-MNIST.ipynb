{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([#transforms.Resize(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_loader))\n",
    "imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# define the CNN architecture\n",
    "class ResNet(nn.Module):\n",
    "    ### choose an architecture, and complete the class\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes, stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) (something like `nn.CrossEntropyLoss` or `nn.NLLLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training:  89.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy before training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in test_loader:\n",
    "\n",
    "    # forward pass to get outputs\n",
    "    # the outputs are a series of class scores\n",
    "    outputs = model(images)\n",
    "\n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # count up total number of correct labels\n",
    "    # for which the predicted and true labels are equal\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# calculate the accuracy\n",
    "# to convert `correct` from a Tensor into a scalar, use .item()\n",
    "accuracy = 100.0 * correct.item() / total\n",
    "\n",
    "# print it out!\n",
    "print('Accuracy before training: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    \n",
    "    loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_i % 100 == 99:    # print every 1000 batches\n",
    "                avg_loss = running_loss/100\n",
    "                # record and print the avg loss over the 1000 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return loss_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Avg. Loss: 0.8255831152200699\n",
      "Epoch: 1, Batch: 200, Avg. Loss: 0.5367798402905464\n",
      "Epoch: 1, Batch: 300, Avg. Loss: 0.48933920711278917\n",
      "Epoch: 1, Batch: 400, Avg. Loss: 0.45551475569605826\n",
      "Epoch: 1, Batch: 500, Avg. Loss: 0.4417376708984375\n",
      "Epoch: 1, Batch: 600, Avg. Loss: 0.43629080429673195\n",
      "Epoch: 1, Batch: 700, Avg. Loss: 0.3958969834446907\n",
      "Epoch: 1, Batch: 800, Avg. Loss: 0.3978364995121956\n",
      "Epoch: 1, Batch: 900, Avg. Loss: 0.3953130167722702\n",
      "Epoch: 2, Batch: 100, Avg. Loss: 0.3796831803023815\n",
      "Epoch: 2, Batch: 200, Avg. Loss: 0.3570930850505829\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.35448864921927453\n",
      "Epoch: 2, Batch: 400, Avg. Loss: 0.3292989619076252\n",
      "Epoch: 2, Batch: 500, Avg. Loss: 0.34965462520718577\n",
      "Epoch: 2, Batch: 600, Avg. Loss: 0.355712264329195\n",
      "Epoch: 2, Batch: 700, Avg. Loss: 0.3249545541405678\n",
      "Epoch: 2, Batch: 800, Avg. Loss: 0.32454435393214226\n",
      "Epoch: 2, Batch: 900, Avg. Loss: 0.3411918719112873\n",
      "Epoch: 3, Batch: 100, Avg. Loss: 0.30104404672980306\n",
      "Epoch: 3, Batch: 200, Avg. Loss: 0.3113793793320656\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.30320170119404793\n",
      "Epoch: 3, Batch: 400, Avg. Loss: 0.30947123989462855\n",
      "Epoch: 3, Batch: 500, Avg. Loss: 0.31558371275663377\n",
      "Epoch: 3, Batch: 600, Avg. Loss: 0.29951949685811996\n",
      "Epoch: 3, Batch: 700, Avg. Loss: 0.2923069669306278\n",
      "Epoch: 3, Batch: 800, Avg. Loss: 0.30066230207681655\n",
      "Epoch: 3, Batch: 900, Avg. Loss: 0.3004668724536896\n",
      "Epoch: 4, Batch: 100, Avg. Loss: 0.2787362502515316\n",
      "Epoch: 4, Batch: 200, Avg. Loss: 0.26327436916530134\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.2712373473495245\n",
      "Epoch: 4, Batch: 400, Avg. Loss: 0.2846720773726702\n",
      "Epoch: 4, Batch: 500, Avg. Loss: 0.288777904137969\n",
      "Epoch: 4, Batch: 600, Avg. Loss: 0.287600529268384\n",
      "Epoch: 4, Batch: 700, Avg. Loss: 0.29082172431051734\n",
      "Epoch: 4, Batch: 800, Avg. Loss: 0.2908695384860039\n",
      "Epoch: 4, Batch: 900, Avg. Loss: 0.27707289926707745\n",
      "Epoch: 5, Batch: 100, Avg. Loss: 0.2642577980458736\n",
      "Epoch: 5, Batch: 200, Avg. Loss: 0.25674190260469915\n",
      "Epoch: 5, Batch: 300, Avg. Loss: 0.27738263182342054\n",
      "Epoch: 5, Batch: 400, Avg. Loss: 0.26118790574371814\n",
      "Epoch: 5, Batch: 500, Avg. Loss: 0.2668625713139772\n",
      "Epoch: 5, Batch: 600, Avg. Loss: 0.2616435328125954\n",
      "Epoch: 5, Batch: 700, Avg. Loss: 0.25146059922873976\n",
      "Epoch: 5, Batch: 800, Avg. Loss: 0.27570098131895066\n",
      "Epoch: 5, Batch: 900, Avg. Loss: 0.2629793676733971\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# define the number of epochs to train for\n",
    "n_epochs = 5 # start small to see if your model works, initially\n",
    "\n",
    "# call train\n",
    "training_loss = train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.307245\n",
      "\n",
      "Test Accuracy of T-shirt/top: 76% (765/1000)\n",
      "Test Accuracy of Trouser: 97% (975/1000)\n",
      "Test Accuracy of Pullover: 83% (830/1000)\n",
      "Test Accuracy of Dress: 91% (913/1000)\n",
      "Test Accuracy of  Coat: 85% (850/1000)\n",
      "Test Accuracy of Sandal: 96% (960/1000)\n",
      "Test Accuracy of Shirt: 74% (742/1000)\n",
      "Test Accuracy of Sneaker: 99% (990/1000)\n",
      "Test Accuracy of   Bag: 97% (977/1000)\n",
      "Test Accuracy of Ankle boot: 91% (913/1000)\n",
      "\n",
      "Test Accuracy (Overall): 89% (8915/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for batch_i, data in enumerate(test_loader):\n",
    "    \n",
    "    # get the input images and their corresponding labels\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # forward pass to get outputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "            \n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((torch.ones(1) / (batch_i + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    # this creates a `correct` Tensor that holds the number of correctly classified images in a batch\n",
    "    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n",
    "    \n",
    "    # calculate test accuracy for *each* object class\n",
    "    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n",
    "    for i in range(len(labels.data)):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
